<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Introduction:</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h3>Introduction:</h3>

<p>Author:         Bogdan Lobodzinski</p>

<p>GitHub repo:        <a href="https://github.com/Lobodzinski/Machine_Learning">https://github.com/Lobodzinski/Machine_Learning</a></p>

<p>description of the project:
Project is a part of the exercises for the course: Practical Machine Learning from Johns Hopkins University
provided by: Jeff Leek, PhD, Roger D. Peng, PhD, Brian Caffo, PhD
Detailed description of the task one can find on the URL:
<a href="https://class.coursera.org/predmachlearn-016/human_grading/view/courses/973763/assessments/4/submissions">https://class.coursera.org/predmachlearn-016/human_grading/view/courses/973763/assessments/4/submissions</a></p>

<p>Detailed description of the project the reader can find in the work [1]</p>

<h4>Access to the data:</h4>

<p>The URL to the training data:<br>
<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a><br>

The URL to the test data:<br>
<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>

<p>The data can be also find in the source of the project: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>.</p>

<h3>Tools:</h3>

<p>For analysis of the data [1] I use:</p>

<p>A) Tree-based prediction method (CART)
r-package: rpart</p>

<p>B) Random Forest approach </p>

<ul>
<li>for description with references therin: <a href="http://en.wikipedia.org/wiki/Random_forest">http://en.wikipedia.org/wiki/Random_forest</a>,</li>
<li>examples of application in r language: 
r-package:  <a href="http://cran.r-project.org/web/packages/randomForest/index.html">http://cran.r-project.org/web/packages/randomForest/index.html</a>,
examples:   <a href="http://www.statmethods.net/advstats/cart.html">http://www.statmethods.net/advstats/cart.html</a></li>
</ul>

<h3>Analysis:</h3>

<h4>1. how you built your model,</h4>

<h5>a) initial seps:</h5>

<pre><code>set.seed(123456)
library(caret)
library(RCurl)
</code></pre>

<p>Download of the training and testing data:</p>

<pre><code>train&lt;-read.csv(textConnection(getURL(&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;)))
test&lt;-read.csv(textConnection(getURL(&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;)))
</code></pre>

<h5>b) Clean-up the data:</h5>

<h6>step 1:</h6>

<p>using </p>

<p><code>featurePlot(x = train[, names1], y = train$classe, plot = &quot;pairs&quot;)</code>
where names1 is a part of list of general list </p>

<p><code>names(train)</code> I selected a few columns which can be removed from the train part. 
Due to the limited RAM, I moved through all column names selecting for each check 10 column names.
I noticed negative response in case of columns:</p>

<p><code>c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))</code></p>

<p>The removal of unrelevant column is:</p>

<pre><code>train1&lt;-subset(train, select=-c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))
test1&lt;-subset(test, select=-c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))
</code></pre>

<h6>step 2:</h6>

<p>check NearZeroVariance variables: </p>

<p><code>nearZeroVar(train1, saveMetrics=TRUE)</code></p>

<p>I selected all columns with <code>nzv=TRUE</code> and removed from the train set:</p>

<pre><code>train2&lt;-subset(train1, select=-c(kurtosis_roll_belt,kurtosis_picth_belt,kurtosis_yaw_belt,skewness_roll_belt,skewness_roll_belt.1,skewness_yaw_belt,max_yaw_belt,
min_yaw_belt,amplitude_yaw_belt,kurtosis_roll_arm,kurtosis_picth_arm,kurtosis_yaw_arm,skewness_roll_arm,skewness_pitch_arm,skewness_yaw_arm,
kurtosis_roll_dumbbell,kurtosis_picth_dumbbell,kurtosis_yaw_dumbbell,skewness_roll_dumbbell,skewness_pitch_dumbbell,skewness_yaw_dumbbell,
max_yaw_dumbbell,min_yaw_dumbbell,amplitude_yaw_dumbbell,kurtosis_roll_forearm,kurtosis_picth_forearm,kurtosis_yaw_forearm,skewness_roll_forearm,
skewness_pitch_forearm,skewness_yaw_forearm,max_yaw_forearm,min_yaw_forearm,amplitude_yaw_forearm))`
dim(train2)
[1] 19622    53

test2&lt;-subset(test1, select=-c(kurtosis_roll_belt,kurtosis_picth_belt,kurtosis_yaw_belt,skewness_roll_belt,skewness_roll_belt.1,skewness_yaw_belt,max_yaw_belt,
min_yaw_belt,amplitude_yaw_belt,kurtosis_roll_arm,kurtosis_picth_arm,kurtosis_yaw_arm,skewness_roll_arm,skewness_pitch_arm,skewness_yaw_arm,
kurtosis_roll_dumbbell,kurtosis_picth_dumbbell,kurtosis_yaw_dumbbell,skewness_roll_dumbbell,skewness_pitch_dumbbell,skewness_yaw_dumbbell,
max_yaw_dumbbell,min_yaw_dumbbell,amplitude_yaw_dumbbell,kurtosis_roll_forearm,kurtosis_picth_forearm,kurtosis_yaw_forearm,skewness_roll_forearm,
skewness_pitch_forearm,skewness_yaw_forearm,max_yaw_forearm,min_yaw_forearm,amplitude_yaw_forearm))`

dim(test2)
</code></pre>

<h5>c) splitting the data:</h5>

<p>training set: trainSet=60 %; 
test set: testSet=40 %;</p>

<pre><code>inTrain &lt;- createDataPartition(y=train$classe, p=0.6, list=FALSE)
trainSet &lt;- train1[inTrain, ]
testSet &lt;- train1[-inTrain, ]
dim(trainSet)
dim(testSet)
</code></pre>

<h4>2. Predictions / how you used cross validation:</h4>

<h5>a) The Tree-based prediction model:</h5>

<pre><code>library(rpart)
m1 &lt;- rpart(classe ~ ., data = trainSet)
library(rattle)
fancyRpartPlot(m1)
</code></pre>

<p>
<img src="https://github.com/Lobodzinski/Machine_Learning/blob/master/TreeRplot_Fig1.png" alt=""/></p>

<h5>b) The Random Forest prediction model:</h5>

<p>The 5-fold cross-validation is performed for the Random Forest prediction method
(<a href="http://topepo.github.io/caret/training.html">http://topepo.github.io/caret/training.html</a>).</p>

<pre><code>fitControl &lt;- trainControl(## 5-fold CV
                           method = &quot;repeatedcv&quot;,
                           number = 5,
                           ## repeated ten times
                           repeats = 5)

m2 &lt;- train(classe ~ ., data = trainSet,
                 method = &quot;rf&quot;,
                 trControl = fitControl,
                 verbose = FALSE)

</code></pre>

<h5>c) Comparison of the prediction methods:</h5>

<p>I compare both prediction method using <code>confusionMatrix</code>.</p>

<p>For the Tree-based prediction algorithm:</p>

<pre><code>Tree &lt;- predict(m1, testSet, type = &quot;class&quot;)
table(testSet$classe, Tree)

   Tree
       A    B    C    D    E
  A 2031   79   65   25   32
  B  212  960  139  138   69
  C   21  107 1112   88   40
  D   68   94  214  830   80
  E   19  140  168   81 1034

Treeconf&lt;-confusionMatrix(Tree, testSet$classe)
Treeconf

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2031  212   21   68   19
         B   79  960  107   94  140
         C   65  139 1112  214  168
         D   25  138   88  830   81
         E   32   69   40   80 1034

Overall Statistics

               Accuracy : 0.7605          
                 95% CI : (0.7509, 0.7699)
    No Information Rate : 0.2845          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       

                  Kappa : 0.6966          
 Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9099   0.6324   0.8129   0.6454   0.7171
Specificity            0.9430   0.9336   0.9095   0.9494   0.9655
Pos Pred Value         0.8639   0.6957   0.6549   0.7143   0.8239
Neg Pred Value         0.9634   0.9137   0.9584   0.9318   0.9381
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2589   0.1224   0.1417   0.1058   0.1318
Detection Prevalence   0.2996   0.1759   0.2164   0.1481   0.1600
Balanced Accuracy      0.9265   0.7830   0.8612   0.7974   0.8413
</code></pre>

<p>In case of the Random Forest method:</p>

<pre><code>Forest &lt;- predict(m2, testSet)
forestconf&lt;-confusionMatrix(Forest, testSet$classe)
forestconf

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2231   16    0    0    1
         B    1 1497    8    0    2
         C    0    5 1351   22    4
         D    0    0    9 1264    3
         E    0    0    0    0 1432

Overall Statistics

               Accuracy : 0.991           
                 95% CI : (0.9886, 0.9929)
    No Information Rate : 0.2845          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       

                  Kappa : 0.9886          
 Mcnemar&#39;s Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9996   0.9862   0.9876   0.9829   0.9931
Specificity            0.9970   0.9983   0.9952   0.9982   1.0000
Pos Pred Value         0.9924   0.9927   0.9776   0.9906   1.0000
Neg Pred Value         0.9998   0.9967   0.9974   0.9967   0.9984
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2843   0.1908   0.1722   0.1611   0.1825
Detection Prevalence   0.2865   0.1922   0.1761   0.1626   0.1825
Balanced Accuracy      0.9983   0.9922   0.9914   0.9905   0.9965
</code></pre>

So, I got:
<pre><code>
            Accuracy:   

Tree  :     0.7605
Forest:     0.991
</code></pre>

<p>The Random Forest method is more reliable then the Tree-based algorithm. Therefore for the 
submission part of the project I will use the The Random Forest method.
Results based on the test data:</p>

<pre><code>forestFit &lt;- predict(m2$finalModel, newdata = test2)
forestFit
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
 B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
Levels: A B C D E
</code></pre>
<h5>Wrinting result to the files:</h5>

<p>on the Linux OS I used the following way. 
&ldquo;
<pre><code>
pml<em>write_files = function(x){
     n = length(x)
     for(i in 1:n){
         filename = paste0(&quot;submission/problem_id</em>&rdquo;,i,&ldquo;.txt&rdquo;)
         write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
     }
}</p>

pml_write_files(forestFit)
</code></pre>

<p>After creation of files, removal of a new line sign inside each file is necessary.
It can be done using <code>zsh</code> loop:</p>

<pre><code>foreach i (`ls`)
echo $i
perl -i -pe &#39;s/\n//g&#39; $i
end
</code></pre>

<p>The size of each file <code>problem_id_*.txt</code> should be 1 . </p>

<h4>3. what you think the expected out of sample error is / Conclusions:</h4>
<p>The Random Forests prediction method generates better results then the Tree-based algorithm.</p>

<h4>4. why you made the choices you did.</h4>

<p>The Tree-based and the Random Forest prediction algorithms are more or less described in available lectures. Also, the Random Forest method is used for the alalysis in the source article [1].  </p>

<h3>References:</h3>

<p>[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.
&ldquo;Qualitative Activity Recognition of Weight Lifting Exercises.&rdquo;
Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human &#39;13) . 
Stuttgart, Germany: ACM SIGCHI, 2013. 
(available on http://groupware.les.inf.puc-rio.br/har)</p>

</body>

</html>
